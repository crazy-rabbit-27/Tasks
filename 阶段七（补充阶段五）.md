## 阶段七（补充阶段五）

1.首先既然要利用Python爬取网站资源，那么必然要了解爬虫是什么？

网上搜索的解释较长，简单来说就是：自动抓取目标网站内容的工具。

2.爬虫的作用是什么呢？

就是提高数据采集效率，我感觉爬虫的本质就是找规律。

（应该没有人想让自己的手指不停的重复复制粘贴的动作，机械性的事情，就应该交给工具去做。）

3.利用python爬取网站资源（借助使用pycharm和BeautifulSoup库进行学习）

根据网络教程，大致有了一下步骤：

（既然是爬取网站资源，那么就先找一个网站）

第一步：获取header和cookie

获取header和cookie是一个爬虫程序必须的，它直接决定了爬虫程序能不能准确的找到网页位置进行爬取。

找到想要爬取的网站，查看源代码（F12）可以看到网页的js语言设计部分，找到网页上的Network部分（如果进行的没有文件信息，就要刷新试试），在我们浏览Name这部分，找到咱们想要爬取的文件再复制下网页的URL。

复制好URL后，我们就进入一个网页！！！Convert curl commands to code！！！（如图）这个网页可以根据你复制的URL，自动生成header和cookie。

![image-20240302192747848](C:\Users\多妹\AppData\Roaming\Typora\typora-user-images\image-20240302192747848.png)

将生成的header和cookie直接复制粘贴回程序中。

有可能会出现这种情况：

只有header没有cookie

解决方法：找有cookie的headers

第二步：用request请求，获取到网页

使用request请求，代码如下述：
response = requests.get('网页链接', headers=headers, params=params, cookies=cookies)

将此段代码放入程序中，就获得到网页了

第三步：

然后，又要返回网页，再次查看源代码（按F12即可），找到网页的Elements部分，有个箭头被框住的图标，将它点击后，再点击网页内容，这个时候网页就会自动在右边显示出你获取网页部分对应的代码。

找到想要爬取的页面部分的网页代码后，再复制到selector部分（copy-->copy selector)

第四步：简化地址

（刚才复制的selector就相当于网页上对应部分存放的地址），将我们所需要的信息进行分析。查阅资料了解到要是只用那个地址的话，就只能获取到你选择的网页上的那部分内容（但是一般的话，我觉得应该是要到全部所需的信息，所以呢）需要用到子类选择器[简单介绍](https://blog.csdn.net/qq_44366571/article/details/101919756)

（通过对比获取的地址有很多相同的地方，唯一不同的地方就是tr部分。由于tr是网页标签，后面的部分就是其补充的部分，也就是子类选择器。该类信息，就是存储在tr的子类中，我们直接对tr进行信息提取，就可以获取到该部分对应的所有信息。）《《《网上较为详细的描述

我觉得简单来说：就是将tr后的内容不写，不将内容描述具体，而是总的概括，这样搜集的范围就会更广；或者更简单的说：就是保留下相同部分。

第五步：

现在就可以直接爬取数据，用一个标签存储上面提炼出的像地址一样的东西。标签就会拉取到我们想获得的网页内容，然后过滤掉我们不需要的信息就可以了。



学习地点：

[参考地1](https://www.jianshu.com/p/f149b1a8e413)

[参考地2](https://cloud.tencent.com/developer/article/2385106?areaId=106001)

[参考地3](https://blog.csdn.net/qq_46094651/article/details/132807373)

[参考视频1](https://www.bilibili.com/video/BV1CY411f7yh/?spm_id_from=333.337.search-card.all.click)

[参考视频2](https://www.bilibili.com/video/BV1ha4y1H7sx/?spm_id_from=333.337.search-card.all.click)

[例子](https://blog.csdn.net/xiangxueerfei/article/details/133379957)
